{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7473bc43-6630-444b-ba8a-178faaf68034",
   "metadata": {},
   "source": [
    "# üß™ Anomaly Scoring & Heatmaps\n",
    "\n",
    "In this step, we transform ViT-based features into meaningful **anomaly scores** and **visual heatmaps**.\n",
    "\n",
    "Using the patch-level embeddings from *test images*, we compare them against the **feature bank** built from `train/good` data. This allows us to detect both image-level anomalies and localize defective regions.\n",
    "\n",
    "---\n",
    "\n",
    "### Goals of this notebook:\n",
    "\n",
    "- Compute **image-level anomaly scores** using:\n",
    "  - **KNN distance** (via FAISS)\n",
    "  - Optionally: **Mahalanobis distance**\n",
    "- Generate **pixel-level heatmaps** by projecting patch distances to spatial grids\n",
    "- Upsample and visualize heatmaps as overlays on the original images\n",
    "- Determine an optimal threshold for binary decisions (Youden‚Äôs J or best F1)\n",
    "- Evaluate detection performance using standard metrics (AUROC, PRO, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "This scoring approach works **fully unsupervised**, relying only on normal (`train/good`) examples.\n",
    "\n",
    "By the end of this notebook, you‚Äôll be able to:\n",
    "- Score all test images\n",
    "- Visualize and interpret heatmaps\n",
    "- Quantify anomaly detection performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acaf7e6-7ddc-42e5-8cfb-095fff5ad973",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Load Features & Metadata\n",
    "\n",
    "In this section, we load the precomputed ViT embeddings and metadata generated during the feature extraction step.\n",
    "\n",
    "Specifically, we will:\n",
    "- Load `.npz` feature files (CLS + patch embeddings) for each category\n",
    "- Load the corresponding `.csv` metadata files\n",
    "- Merge them into a single DataFrame for easy access and filtering\n",
    "\n",
    "This unified structure will allow us to:\n",
    "- Quickly isolate `train/good` features (for scoring)\n",
    "- Select `test` images for evaluation\n",
    "- Link embeddings back to image paths for visualization\n",
    "\n",
    "---\n",
    "\n",
    "Each entry contains:\n",
    "- `cls`: global image embedding (ViT CLS token)\n",
    "- `patches`: local patch embeddings\n",
    "- `patch_hw`: patch grid shape (e.g., 16√ó16)\n",
    "- metadata (path, label, split, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b99c335-c6b1-45da-8ad6-06cb0a6962bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved bottle (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved cable (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved capsule (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved carpet (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved grid (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved hazelnut (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved leather (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved metal_nut (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved pill (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved screw (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved tile (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved toothbrush (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved transistor (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved wood (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved zipper (dino) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\dino\n",
      "‚úÖ Saved bottle (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved cable (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved capsule (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved carpet (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved grid (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved hazelnut (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved leather (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved metal_nut (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved pill (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved screw (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved tile (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved toothbrush (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved transistor (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved wood (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n",
      "‚úÖ Saved zipper (mae) to C:\\Users\\Fredi\\MVTec\\cached_dicts\\mae\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "BASE_DIR = Path(\"..\").resolve()\n",
    "\n",
    "CFG = {\n",
    "    \"dino\": {\n",
    "        \"FEAT_DIR\": BASE_DIR / \"features_dinov2_b14\",\n",
    "        \"BANK_DIR\": BASE_DIR / \"featurebanks\" / \"dinov2_b14\",\n",
    "        \"FEAT_NPZ_TPL\": \"{cat}_dinov2_vitb14.npz\",\n",
    "        \"BANK_NPZ_TPL\": \"{cat}_bank_dinov2_b14.npz\",\n",
    "    },\n",
    "    \"mae\": {\n",
    "        \"FEAT_DIR\": BASE_DIR / \"features_mae_b16\",\n",
    "        \"BANK_DIR\": BASE_DIR / \"featurebanks\" / \"mae_b16\",\n",
    "        \"FEAT_NPZ_TPL\": \"{cat}_mae_b16.npz\",\n",
    "        \"BANK_NPZ_TPL\": \"{cat}_bank_mae_b16.npz\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def categories_from_feat_dir(feat_dir: Path):\n",
    "    cats = []\n",
    "    for p in feat_dir.glob(\"*_meta.csv\"):\n",
    "        name = p.name\n",
    "        if name.endswith(\"_meta.csv\"):\n",
    "            cats.append(name[:-len(\"_meta.csv\")])\n",
    "    return sorted(set(cats))\n",
    "\n",
    "def load_feature_data(feat_dir: Path, feat_tpl: str, category: str):\n",
    "    feat_file = feat_dir / feat_tpl.format(cat=category)\n",
    "    meta_file = feat_dir / f\"{category}_meta.csv\"\n",
    "    data = np.load(feat_file)\n",
    "    df = pd.read_csv(meta_file)\n",
    "    return {\n",
    "        \"patches\": data[\"patches\"],\n",
    "        \"cls\": data.get(\"cls\"),\n",
    "        \"patch_hw\": tuple(data[\"patch_hw\"]),\n",
    "        \"meta\": df\n",
    "    }\n",
    "\n",
    "def load_feature_bank(bank_dir: Path, bank_tpl: str, category: str):\n",
    "    bank_file = bank_dir / bank_tpl.format(cat=category)\n",
    "    meta_file = bank_dir / f\"{category}_bank_meta.csv\"\n",
    "    data = np.load(bank_file)\n",
    "    df = pd.read_csv(meta_file)\n",
    "    return {\n",
    "        \"patches\": data[\"patches\"],\n",
    "        \"patch_hw\": tuple(data[\"patch_hw\"]),\n",
    "        \"meta\": df\n",
    "    }\n",
    "\n",
    "def save_dicts_to_disk(backbone: str, features_all: dict, banks: dict, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for cat in features_all:\n",
    "        with open(out_dir / f\"{cat}_features.pkl\", \"wb\") as f:\n",
    "            pickle.dump(features_all[cat], f)\n",
    "        with open(out_dir / f\"{cat}_bank.pkl\", \"wb\") as f:\n",
    "            pickle.dump(banks[cat], f)\n",
    "        print(f\"‚úÖ Saved {cat} ({backbone}) to {out_dir}\")\n",
    "\n",
    "def load_backbone(name: str):\n",
    "    cfg = CFG[name]\n",
    "    feat_dir, bank_dir = cfg[\"FEAT_DIR\"], cfg[\"BANK_DIR\"]\n",
    "    feat_tpl, bank_tpl = cfg[\"FEAT_NPZ_TPL\"], cfg[\"BANK_NPZ_TPL\"]\n",
    "\n",
    "    categories = categories_from_feat_dir(feat_dir)\n",
    "    features_all, banks = {}, {}\n",
    "    for cat in categories:\n",
    "        features_all[cat] = load_feature_data(feat_dir, feat_tpl, cat)\n",
    "        banks[cat] = load_feature_bank(bank_dir, bank_tpl, cat)\n",
    "    return categories, features_all, banks\n",
    "\n",
    "# Load and save DINO\n",
    "cats_dino, features_all_dino, banks_dino = load_backbone(\"dino\")\n",
    "save_dicts_to_disk(\"dino\", features_all_dino, banks_dino, BASE_DIR / \"cached_dicts\" / \"dino\")\n",
    "\n",
    "# Load and save MAE\n",
    "cats_mae, features_all_mae, banks_mae = load_backbone(\"mae\")\n",
    "save_dicts_to_disk(\"mae\", features_all_mae, banks_mae, BASE_DIR / \"cached_dicts\" / \"mae\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0432845d-b4be-4b4b-8a5d-722013aecfb7",
   "metadata": {},
   "source": [
    "### Data Structures\n",
    "\n",
    "After loading, we store the results in two dictionaries.  \n",
    "This design separates **all extracted embeddings** from the **reference banks**,  \n",
    "making evaluation, visualization, and anomaly scoring more convenient.\n",
    "\n",
    "---\n",
    "\n",
    "- **`features_all`**  \n",
    "  Contains the **full embeddings** (train + test) for each category.  \n",
    "  Each entry is a dictionary with:\n",
    "  - `cls`: global image embeddings `[N, D]`\n",
    "  - `patches`: patch embeddings `[N, P, D]`\n",
    "  - `patch_hw`: patch grid dimensions `(H, W)`\n",
    "  - `meta`: metadata DataFrame (paths, labels, splits)\n",
    "\n",
    "  ‚Üí Used when we need to compare across *all images* (e.g., evaluation, visualization).\n",
    "\n",
    "---\n",
    "\n",
    "- **`banks`**  \n",
    "  Contains only the **feature bank**: patch embeddings from `train/good` images.  \n",
    "  Each entry is a dictionary with:\n",
    "  - `patches`: patch embeddings `[N, P, D]`\n",
    "  - `patch_hw`: patch grid dimensions `(H, W)`\n",
    "  - `meta`: metadata DataFrame for good samples\n",
    "\n",
    "  ‚Üí Used as the **reference set** for anomaly scoring: test patches are compared against this bank.\n",
    "\n",
    "\n",
    "These structures allow us to seamlessly switch between global evaluation (`features_all`) and reference-based scoring (`banks`). In the next step, we leverage them to compute image-level anomaly scores and pixel-level heatmaps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee1e6e-d2e7-4788-bcd4-f2ff721a224d",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Build Search Index & Image-Level Scoring\n",
    "\n",
    "After extracting features, the next step is to organize the  \n",
    "**reference distribution of normal data** and to perform  \n",
    "**image-level anomaly scoring** based on this reference.  \n",
    "This combined stage forms the *core* of the anomaly detection pipeline.\n",
    "\n",
    "Two complementary approaches are implemented:\n",
    "\n",
    "- **KNN-based feature banks (non-parametric):**  \n",
    "  Patch embeddings from `train/good` images are stored in a  \n",
    "  nearest-neighbor index (FAISS, cosine similarity with PCA to 128D).  \n",
    "  ‚Üí At inference, each test patch is compared to its closest neighbors  \n",
    "  in this bank.\n",
    "\n",
    "- **Mahalanobis distribution (parametric):**  \n",
    "  For each category, the mean vector (Œº) and a shrunk covariance matrix (Œ£ÃÇ)  \n",
    "  of `train/good` patches are estimated.  \n",
    "  ‚Üí Anomaly scores are then derived from the squared Mahalanobis distance,  \n",
    "  measuring how well test patches fit into the global distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### Image-Level Scoring\n",
    "\n",
    "For every test image:\n",
    "\n",
    "1. **Patch-level comparison:**  \n",
    "   Each patch embedding is compared against the chosen reference  \n",
    "   (nearest neighbors for KNN, multivariate distance for Mahalanobis).  \n",
    "\n",
    "2. **Patch scoring:**  \n",
    "   Each patch receives an anomaly score (distance).  \n",
    "\n",
    "3. **Image aggregation:**  \n",
    "   The **Top-K most anomalous patches** are averaged, yielding  \n",
    "   a single **image-level anomaly score**.  \n",
    "\n",
    "---\n",
    "\n",
    "By uniting the **construction of the normal reference** with the  \n",
    "**aggregation of patch scores into image-level scores**,  \n",
    "this stage transforms raw features into **directly usable anomaly scores**.  \n",
    "These scores provide the basis for evaluation, threshold selection,  \n",
    "and visualization in later steps.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33cce3-e735-40c0-9236-9f95363f1b19",
   "metadata": {},
   "source": [
    "### üîπ2.1 KNN Implementation (with FAISS) \n",
    "\n",
    "In our pipeline, the **KNN-based anomaly scoring** is implemented as a \n",
    "streaming procedure to stay RAM-safe while handling large feature banks.  \n",
    "The core logic is encapsulated in [`src/scoring_knn.py`](src/scoring_knn.py).\n",
    "\n",
    "**Workflow:**\n",
    "1. **Feature Bank Creation:**  \n",
    "   All patch embeddings from `train/good` images are stored as `.pkl` files \n",
    "   (one `*_features.pkl` for metadata + test/train splits, one `*_bank.pkl` \n",
    "   containing the reference patches).\n",
    "\n",
    "2. **Streaming per Category:**  \n",
    "   Instead of loading the entire dataset into memory, \n",
    "   `score_backbone_streaming()` processes one category at a time.  \n",
    "   For each category:\n",
    "   - Load the feature bank (`train/good` patches).  \n",
    "   - Apply **PCA reduction to 128 dimensions** (from the original 768D).  \n",
    "     ‚Üí This lowers memory usage and speeds up FAISS queries, while \n",
    "       retaining most discriminative information.  \n",
    "   - Build a **FAISS index** with **cosine similarity** as metric.  \n",
    "\n",
    "3. **Patch-Level k-NN Search:**  \n",
    "   Each test patch is queried against the index.  \n",
    "   - We use **cosine similarity** instead of L2 distance, because \n",
    "     embeddings from pre-trained ViTs (DINO/MAE) are generally \n",
    "     normalized and directional information is more robust for \n",
    "     anomaly detection.  \n",
    "   - Anomaly distance = `1 - cosine_similarity`.  \n",
    "   - For each patch, the **mean distance of its k nearest neighbors** is computed.  \n",
    "\n",
    "4. **Image-Level Aggregation:**  \n",
    "   From the patch-level scores, the top-K most anomalous patches are \n",
    "   averaged ‚Üí this yields the **final image-level anomaly score**.  \n",
    "\n",
    "5. **Output:**  \n",
    "   Results are appended into a CSV file, one row per test image:  \n",
    "\n",
    "   | idx | path | category | raw_label | label | image_score |  \n",
    "   |-----|------|----------|-----------|-------|-------------|  \n",
    "\n",
    "**Advantages:**\n",
    "- Memory-efficient (streaming, batch search).  \n",
    "- PCA (128D) greatly reduces RAM and compute requirements.  \n",
    "- Cosine similarity aligns better with the embedding geometry of \n",
    "  transformers, leading to more stable scoring.  \n",
    "- GPU-accelerated nearest neighbor queries via FAISS.  \n",
    "\n",
    "This implementation ensures that anomaly scoring remains **scalable and efficient**, \n",
    "while preserving the accuracy benefits of non-parametric k-NN methods.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59e2b0a9-9269-41f6-8d94-7260082fb892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx              int64\n",
      "path            object\n",
      "category        object\n",
      "raw_label       object\n",
      "label           object\n",
      "image_score    float64\n",
      "dtype: object\n",
      "0    0.357246\n",
      "1    0.343680\n",
      "2    0.337821\n",
      "3    0.422264\n",
      "4    0.369904\n",
      "Name: image_score, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1725.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.177325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.093315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.036036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.095347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.164082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.237266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.641549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_score\n",
       "count  1725.000000\n",
       "mean      0.177325\n",
       "std       0.093315\n",
       "min       0.036036\n",
       "25%       0.095347\n",
       "50%       0.164082\n",
       "75%       0.237266\n",
       "max       0.641549"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Mean anomaly score per label (cleaned):\n",
      "label\n",
      "defect    0.207137\n",
      "good      0.097017\n",
      "Name: image_score, dtype: float64\n",
      "\n",
      "üìà Difference (defect - good): 0.1101\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV laden, zur Sicherheit Separator explizit angeben\n",
    "df = pd.read_csv(\"../scores_knn/scores_STREAM_dino_cosine_k5_top5.csv\", sep=\",\")\n",
    "\n",
    "# image_score in float konvertieren\n",
    "df[\"image_score\"] = pd.to_numeric(df[\"image_score\"], errors=\"coerce\")\n",
    "\n",
    "# Kurzer Check\n",
    "print(df.dtypes)\n",
    "print(df[\"image_score\"].head())\n",
    "\n",
    "# Statistical summary\n",
    "display(df.describe()[[\"image_score\"]])\n",
    "\n",
    "# Nur \"good\" und \"defect\" ber√ºcksichtigen\n",
    "mean_scores = df[df[\"label\"].isin([\"good\", \"defect\"])] \\\n",
    "    .groupby(\"label\")[\"image_score\"].mean()\n",
    "\n",
    "print(\"\\nüîç Mean anomaly score per label (cleaned):\")\n",
    "print(mean_scores)\n",
    "\n",
    "if {\"good\", \"defect\"} <= set(mean_scores.index):\n",
    "    delta = mean_scores[\"defect\"] - mean_scores[\"good\"]\n",
    "    print(f\"\\nüìà Difference (defect - good): {delta:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "109525fa-7c17-403a-9778-aa926ff8411a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Mean anomaly score per label (KNN):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DINO (KNN)</th>\n",
       "      <th>MAE (KNN)</th>\n",
       "      <th>Œî (defect - good) DINO</th>\n",
       "      <th>Œî (defect - good) MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.097017</td>\n",
       "      <td>0.148898</td>\n",
       "      <td>0.11012</td>\n",
       "      <td>0.034547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>defect</th>\n",
       "      <td>0.207137</td>\n",
       "      <td>0.183444</td>\n",
       "      <td>0.11012</td>\n",
       "      <td>0.034547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DINO (KNN)  MAE (KNN)  Œî (defect - good) DINO  Œî (defect - good) MAE\n",
       "label                                                                       \n",
       "good      0.097017   0.148898                 0.11012               0.034547\n",
       "defect    0.207137   0.183444                 0.11012               0.034547"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Score difference (defect - good):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DINO (KNN)    0.110120\n",
       "MAE (KNN)     0.034547\n",
       "Name: Œî (defect - good), dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths ---\n",
    "p_dino = Path(\"../scores_knn/scores_STREAM_dino_cosine_k5_top5.csv\")\n",
    "p_mae  = Path(\"../scores_knn/scores_STREAM_mae_cosine_k5_top5.csv\")\n",
    "\n",
    "# --- Load + clean helper ---\n",
    "def load_clean_scores(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    # Ensure numeric (in case it's stored as string)\n",
    "    if df[\"image_score\"].dtype != \"float\":\n",
    "        df[\"image_score\"] = pd.to_numeric(df[\"image_score\"], errors=\"coerce\")\n",
    "    # Keep only valid labels\n",
    "    df = df[df[\"label\"].isin([\"good\", \"defect\"])].copy()\n",
    "    # Drop corrupted rows if any\n",
    "    df = df.dropna(subset=[\"image_score\", \"label\"])\n",
    "    return df\n",
    "\n",
    "df_dino = load_clean_scores(p_dino)\n",
    "df_mae  = load_clean_scores(p_mae)\n",
    "\n",
    "# --- Mean per label ---\n",
    "mean_dino = df_dino.groupby(\"label\")[\"image_score\"].mean()\n",
    "mean_mae  = df_mae.groupby(\"label\")[\"image_score\"].mean()\n",
    "\n",
    "# Align index to ensure consistent order\n",
    "label_order = [\"good\", \"defect\"]\n",
    "mean_dino = mean_dino.reindex(label_order)\n",
    "mean_mae  = mean_mae.reindex(label_order)\n",
    "\n",
    "# --- Merge + deltas ---\n",
    "mean_df = pd.DataFrame({\n",
    "    \"DINO (KNN)\": mean_dino,\n",
    "    \"MAE (KNN)\":  mean_mae\n",
    "})\n",
    "mean_df[\"Œî (defect - good) DINO\"] = mean_dino.loc[\"defect\"] - mean_dino.loc[\"good\"]\n",
    "mean_df[\"Œî (defect - good) MAE\"]  = mean_mae.loc[\"defect\"]  - mean_mae.loc[\"good\"]\n",
    "\n",
    "print(\"üîç Mean anomaly score per label (KNN):\")\n",
    "display(mean_df)\n",
    "\n",
    "# Compact delta-only output\n",
    "delta = pd.Series({\n",
    "    \"DINO (KNN)\": mean_dino.loc[\"defect\"] - mean_dino.loc[\"good\"],\n",
    "    \"MAE (KNN)\":  mean_mae.loc[\"defect\"]  - mean_mae.loc[\"good\"],\n",
    "}, name=\"Œî (defect - good)\")\n",
    "print(\"\\nüìà Score difference (defect - good):\")\n",
    "display(delta)\n",
    "\n",
    "# --- Optional: per-category check (uncomment display to view) ---\n",
    "per_cat = (\n",
    "    pd.concat(\n",
    "        [df_dino.assign(model=\"DINO\"), df_mae.assign(model=\"MAE\")],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    .groupby([\"model\", \"category\", \"label\"])[\"image_score\"].mean()\n",
    "    .unstack(\"label\")\n",
    "    .assign(delta=lambda x: x[\"defect\"] - x[\"good\"])\n",
    "    .sort_values([\"model\", \"delta\"], ascending=[True, False])\n",
    ")\n",
    "# display(per_cat)  # Uncomment if you want to inspect per-category deltas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef53ab5-575c-48f7-a6ed-e73c1aa3108f",
   "metadata": {},
   "source": [
    "\n",
    "#### üìä Observed Score Distribution (DINO, cosine, PCA=128D, k=5, TopK=5)\n",
    "\n",
    "*The following results were obtained by running the pipeline in **Google Colab**.*\n",
    "\n",
    "**Global stats (all test images, N=1,725):**\n",
    "- count: **1725**\n",
    "- mean: **0.1773**\n",
    "- std: **0.0933**\n",
    "- min / 25% / 50% / 75% / max: **0.0361 / 0.0953 / 0.1641 / 0.2373 / 0.6416**\n",
    "\n",
    "**By label:**\n",
    "- good ‚Üí mean **0.0970**\n",
    "- defect ‚Üí mean **0.2071**\n",
    "\n",
    "**Interpretation:**\n",
    "- Scores are clearly **higher for defective images** (‚âà2√ó on average), which indicates good separability.\n",
    "- The overall spread (up to ~0.64) shows that strongly anomalous cases stand out clearly.\n",
    "- A practical starting **threshold** typically falls between **0.12‚Äì0.18**  \n",
    "  (more precisely determined via ROC; e.g., Youden‚Äôs J = argmax(tpr ‚àí fpr)).\n",
    "\n",
    "**Implications for downstream evaluation:**\n",
    "- A **high AUROC** (often ‚â•0.9) can be expected given this mean gap.  \n",
    "- For deployment: thresholds should be calibrated per category (e.g., ROC-based or using the 95th percentile of *good* scores).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b0e3d-8d85-42b8-ada5-9505a9e84fa2",
   "metadata": {},
   "source": [
    "### üîπ 2.2 Mahalanobis Implementation\n",
    "\n",
    "In addition to the non-parametric k-NN approach, we also evaluate a **parametric anomaly scoring method** based on the **Mahalanobis distance**.  \n",
    "Here, the assumption is that the embeddings of *normal* patches (from `train/good`) approximately follow a **multivariate Gaussian distribution**.\n",
    "\n",
    "**Workflow:**\n",
    "1. **Fit Distribution:**  \n",
    "   - Collect all patch embeddings from `train/good`.  \n",
    "   - Compute the **mean vector** (Œº) and a **shrunk covariance matrix** (Œ£ÃÇ).  \n",
    "   - Shrinkage (convex combination with identity) ensures stable inversion even when samples are fewer than dimensions or features are correlated.  \n",
    "\n",
    "2. **O PCA (128D):**  \n",
    "   To reduce dimensionality and improve numerical stability, PCA is trained on a sample of patches and applied to both train and test embeddings.  \n",
    "\n",
    "3. **Patch-Level Scoring:**  \n",
    "   Each test patch is assigned a score equal to its **squared Mahalanobis distance**:  \n",
    "   $d^2(x) = (x - \\mu)^T \\hat{\\Sigma}^{-1} (x - \\mu)$\n",
    "\n",
    "\n",
    "4. **Image-Level Aggregation:**  \n",
    "   For each test image, the **Top-K highest patch scores** are averaged, yielding the final **image-level anomaly score**.  \n",
    "\n",
    "**Advantages:**\n",
    "- Parametric: explicitly models the distribution of normal data.  \n",
    "- More compact than storing all patches (no large FAISS index required).  \n",
    "- Shrinkage + PCA improve stability and reduce overfitting.  \n",
    "\n",
    "This method provides a **complementary perspective** to k-NN: instead of comparing to individual neighbors, it measures how well a sample fits into the global distribution of normal features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d012fc9-12ad-4112-bd6f-f850ac8c2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ----------------- PCA + Mahalanobis Core ----------------- #\n",
    "def train_pca_on_sample(cache_dir, backbone, categories, pca_dim, sample_cap=200_000, whitening_power=0.0):\n",
    "    if pca_dim is None:\n",
    "        return None\n",
    "    print(f\"‚è≥ PCA: {backbone}, dim={pca_dim}\")\n",
    "    sample_cat = categories[0]\n",
    "    with open(cache_dir / backbone / f\"{sample_cat}_bank.pkl\", \"rb\") as f:\n",
    "        bank = pickle.load(f)\n",
    "    sb = bank[\"patches\"].reshape(-1, bank[\"patches\"].shape[-1]).astype(np.float32)\n",
    "    idx = np.random.choice(len(sb), size=min(sample_cap, len(sb)), replace=False)\n",
    "    whiten = whitening_power < 0\n",
    "    pca = PCA(n_components=pca_dim, whiten=whiten)\n",
    "    pca.fit(sb[idx])\n",
    "    return pca\n",
    "\n",
    "def apply_pca_inplace(x: np.ndarray, pca: Optional[PCA]) -> np.ndarray:\n",
    "    if pca is None:\n",
    "        return np.ascontiguousarray(x.astype(np.float32, copy=False))\n",
    "    y = pca.transform(x)\n",
    "    return np.ascontiguousarray(y.astype(np.float32, copy=False))\n",
    "\n",
    "def fit_mean_cov_shrunk(X: np.ndarray, shrinkage_alpha: float = 0.1):\n",
    "    mu = X.mean(axis=0)\n",
    "    Xc = X - mu\n",
    "    S = np.cov(Xc, rowvar=False, ddof=1).astype(np.float64)\n",
    "    trace = np.trace(S) if np.isfinite(np.trace(S)) else np.sum(np.var(X, axis=0))\n",
    "    lam = trace / max(S.shape[0], 1)\n",
    "    S_hat = (1.0 - shrinkage_alpha) * S + shrinkage_alpha * lam * np.eye(S.shape[0])\n",
    "    return mu.astype(np.float32), S_hat.astype(np.float32)\n",
    "\n",
    "def invert_posdef(S: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        L = np.linalg.cholesky(S)\n",
    "        Linv = np.linalg.solve(L, np.eye(L.shape[0]))\n",
    "        return (Linv.T @ Linv).astype(np.float32)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.linalg.pinv(S, rcond=1e-6).astype(np.float32)\n",
    "\n",
    "def mahalanobis_sq_batch(X: np.ndarray, mu: np.ndarray, S_inv: np.ndarray, batch_size: int = 50_000):\n",
    "    out = np.empty(X.shape[0], dtype=np.float32)\n",
    "    for s in range(0, X.shape[0], batch_size):\n",
    "        e = min(s + batch_size, X.shape[0])\n",
    "        Xc = X[s:e] - mu\n",
    "        out[s:e] = np.einsum(\"ij,jk,ik->i\", Xc, S_inv, Xc, optimize=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d976177-1079-4520-9fe4-6ea563f56122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import time, gc, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- helpers ---\n",
    "def _infer_hw(P: int) -> Tuple[int, int]:\n",
    "    r = int(np.sqrt(P))\n",
    "    return (r, r) if r * r == P else (1, P)\n",
    "\n",
    "def _save_path_for_npz(patch_out_root: Path, backbone: str, meta_row) -> Path:\n",
    "    \"\"\"\n",
    "    Mirror dataset structure to avoid filename collisions:\n",
    "    <root>/<backbone>/<category>/<split>/<raw_label>/<filename>.png.npz\n",
    "    \"\"\"\n",
    "    p = Path(str(meta_row[\"path\"]))\n",
    "    cat   = str(meta_row[\"category\"])\n",
    "    split = str(meta_row[\"split\"]).lower()\n",
    "    raw   = str(meta_row[\"raw_label\"])\n",
    "    fname = p.name  # e.g. \"000.png\"\n",
    "    save_dir = patch_out_root / backbone / cat / split / raw\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return save_dir / f\"{fname}.npz\"\n",
    "\n",
    "# --- main ---\n",
    "def score_all_backbones_mahalanobis_separate(\n",
    "    cache_dir: Path,\n",
    "    out_files: Dict[str, Path],             # backbone -> CSV path\n",
    "    backbones: List[str],                   # e.g. [\"dino\", \"mae\"]\n",
    "    k_top_patches: int = 5,\n",
    "    pca_dim: Optional[int] = 128,\n",
    "    pca_whitening_power: float = 0.0,\n",
    "    shrinkage_alpha: float = 0.1,\n",
    "    query_batch_size: int = 20_000,\n",
    "    save_patch_scores: bool = True,\n",
    "    patch_out_root: Optional[Path] = None,\n",
    "):\n",
    "    for backbone in backbones:\n",
    "        out_file = out_files[backbone]\n",
    "        out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        wrote_header = False\n",
    "\n",
    "        b_dir = cache_dir / backbone\n",
    "        categories = sorted(p.stem.replace(\"_features\", \"\") for p in b_dir.glob(\"*_features.pkl\"))\n",
    "        print(f\"\\n== [{backbone.upper()}] {len(categories)} categories ==\")\n",
    "\n",
    "        # Train PCA on first category (on the patch bank)\n",
    "        pca = train_pca_on_sample(cache_dir, backbone, categories, pca_dim, whitening_power=pca_whitening_power)\n",
    "\n",
    "        for cat in categories:\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Load features and bank\n",
    "            with open(b_dir / f\"{cat}_features.pkl\", \"rb\") as f: feats = pickle.load(f)\n",
    "            with open(b_dir / f\"{cat}_bank.pkl\", \"rb\") as f: bank = pickle.load(f)\n",
    "\n",
    "            # Fit distribution (mean & shrunk covariance)\n",
    "            bank_mat = bank[\"patches\"].reshape(-1, bank[\"patches\"].shape[-1])\n",
    "            bank_mat = apply_pca_inplace(bank_mat, pca)\n",
    "            mu, S_hat = fit_mean_cov_shrunk(bank_mat, shrinkage_alpha=shrinkage_alpha)\n",
    "            S_inv = invert_posdef(S_hat)\n",
    "\n",
    "            # Extract test patch features\n",
    "            patches = feats[\"patches\"].astype(np.float32)  # [N, P, D]\n",
    "            meta = feats[\"meta\"]\n",
    "            is_test = meta[\"split\"].astype(str).str.lower().eq(\"test\").values\n",
    "            test_idx = np.where(is_test)[0]\n",
    "            P, D = patches.shape[1], patches.shape[2]\n",
    "            X = patches[is_test].reshape(-1, D)\n",
    "            X = apply_pca_inplace(X, pca)\n",
    "\n",
    "            # Mahalanobis distances per patch\n",
    "            patch_scores = mahalanobis_sq_batch(X, mu, S_inv, batch_size=query_batch_size).reshape(-1, P)\n",
    "\n",
    "            # Top-K aggregation per image\n",
    "            tk = min(k_top_patches, P)\n",
    "            order = np.argsort(patch_scores, axis=1)\n",
    "            topk_idx = order[:, -tk:]\n",
    "            img_scores = patch_scores[np.arange(len(patch_scores))[:, None], topk_idx].mean(axis=1)\n",
    "\n",
    "            # Save per-image patch scores (UNIQUE paths)\n",
    "            if save_patch_scores and patch_out_root is not None:\n",
    "                H, W = _infer_hw(P)\n",
    "                for j, i in enumerate(test_idx):\n",
    "                    save_path = _save_path_for_npz(patch_out_root, backbone, meta.iloc[i])\n",
    "                    np.savez_compressed(\n",
    "                        save_path,\n",
    "                        patch_scores=patch_scores[j],                # (P,)\n",
    "                        image_score=float(img_scores[j]),           # scalar\n",
    "                        grid_hw=np.array([H, W], dtype=np.int32),   # e.g., [16, 16]\n",
    "                        topk_idx=topk_idx[j].astype(np.int32),\n",
    "                        meta=dict(\n",
    "                            path=str(meta.iloc[i][\"path\"]),\n",
    "                            category=str(meta.iloc[i][\"category\"]),\n",
    "                            split=str(meta.iloc[i][\"split\"]),\n",
    "                            raw_label=str(meta.iloc[i][\"raw_label\"]),\n",
    "                            label=str(meta.iloc[i][\"label\"]),\n",
    "                            backbone=str(backbone),\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "            # Append CSV\n",
    "            rows = []\n",
    "            for j, i in enumerate(test_idx):\n",
    "                rows.append({\n",
    "                    \"idx\": int(i),\n",
    "                    \"category\": meta.iloc[i][\"category\"],\n",
    "                    \"path\": meta.iloc[i][\"path\"],\n",
    "                    \"raw_label\": meta.iloc[i][\"raw_label\"],\n",
    "                    \"label\": meta.iloc[i][\"label\"],\n",
    "                    \"image_score\": float(img_scores[j]),\n",
    "                })\n",
    "            df = pd.DataFrame(rows)\n",
    "            df.to_csv(out_file, mode=\"a\", header=not wrote_header, index=False)\n",
    "            wrote_header = True\n",
    "\n",
    "            # Cleanup\n",
    "            del feats, bank, bank_mat, mu, S_hat, S_inv, patches, X, patch_scores, img_scores, df\n",
    "            gc.collect()\n",
    "\n",
    "            print(f\"‚Ä¢ {backbone}/{cat}: {len(rows)} imgs in {time.time() - t0:.1f}s\")\n",
    "\n",
    "        print(f\"üìÑ Done ‚Üí {out_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fe4c9a9-0783-427d-978e-f154a2983ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== [DINO] 15 categories ==\n",
      "‚è≥ PCA: dino, dim=128\n",
      "‚Ä¢ dino/bottle: 83 imgs in 0.4s\n",
      "‚Ä¢ dino/cable: 150 imgs in 0.6s\n",
      "‚Ä¢ dino/capsule: 132 imgs in 0.5s\n",
      "‚Ä¢ dino/carpet: 117 imgs in 0.6s\n",
      "‚Ä¢ dino/grid: 78 imgs in 0.5s\n",
      "‚Ä¢ dino/hazelnut: 110 imgs in 0.7s\n",
      "‚Ä¢ dino/leather: 124 imgs in 0.6s\n",
      "‚Ä¢ dino/metal_nut: 115 imgs in 0.5s\n",
      "‚Ä¢ dino/pill: 167 imgs in 0.7s\n",
      "‚Ä¢ dino/screw: 160 imgs in 0.7s\n",
      "‚Ä¢ dino/tile: 117 imgs in 0.5s\n",
      "‚Ä¢ dino/toothbrush: 42 imgs in 0.2s\n",
      "‚Ä¢ dino/transistor: 100 imgs in 0.5s\n",
      "‚Ä¢ dino/wood: 79 imgs in 0.5s\n",
      "‚Ä¢ dino/zipper: 151 imgs in 0.6s\n",
      "üìÑ Done ‚Üí ..\\scores_mahalanobis\\scores_mahalanobis_dino.csv\n",
      "\n",
      "== [MAE] 15 categories ==\n",
      "‚è≥ PCA: mae, dim=128\n",
      "‚Ä¢ mae/bottle: 83 imgs in 0.4s\n",
      "‚Ä¢ mae/cable: 150 imgs in 0.6s\n",
      "‚Ä¢ mae/capsule: 132 imgs in 0.6s\n",
      "‚Ä¢ mae/carpet: 117 imgs in 0.6s\n",
      "‚Ä¢ mae/grid: 78 imgs in 0.5s\n",
      "‚Ä¢ mae/hazelnut: 110 imgs in 0.7s\n",
      "‚Ä¢ mae/leather: 124 imgs in 0.6s\n",
      "‚Ä¢ mae/metal_nut: 115 imgs in 0.5s\n",
      "‚Ä¢ mae/pill: 167 imgs in 0.7s\n",
      "‚Ä¢ mae/screw: 160 imgs in 0.7s\n",
      "‚Ä¢ mae/tile: 117 imgs in 0.5s\n",
      "‚Ä¢ mae/toothbrush: 42 imgs in 0.2s\n",
      "‚Ä¢ mae/transistor: 100 imgs in 0.5s\n",
      "‚Ä¢ mae/wood: 79 imgs in 0.5s\n",
      "‚Ä¢ mae/zipper: 151 imgs in 0.6s\n",
      "üìÑ Done ‚Üí ..\\scores_mahalanobis\\scores_mahalanobis_mae.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Shared configuration\n",
    "CACHE_DIR = Path(\"../cached_dicts\")\n",
    "PATCH_OUT_ROOT = Path(\"../scores_mahalanobis/patch_scores\")\n",
    "PCA_DIM = 128\n",
    "K_TOP_PATCHES = 5\n",
    "BATCH_SIZE = 20_000\n",
    "SHRINKAGE = 0.1\n",
    "WHITENING = 0.0\n",
    "\n",
    "# --- Run 1: DINO ---\n",
    "score_all_backbones_mahalanobis_separate(\n",
    "    cache_dir=CACHE_DIR,\n",
    "    out_files={\n",
    "        \"dino\": Path(\"../scores_mahalanobis/scores_mahalanobis_dino.csv\")\n",
    "    },\n",
    "    backbones=[\"dino\"],\n",
    "    k_top_patches=K_TOP_PATCHES,\n",
    "    pca_dim=PCA_DIM,\n",
    "    pca_whitening_power=WHITENING,\n",
    "    shrinkage_alpha=SHRINKAGE,\n",
    "    query_batch_size=BATCH_SIZE,\n",
    "    save_patch_scores=True,\n",
    "    patch_out_root=PATCH_OUT_ROOT,\n",
    ")\n",
    "\n",
    "# --- Run 2: MAE ---\n",
    "score_all_backbones_mahalanobis_separate(\n",
    "    cache_dir=CACHE_DIR,\n",
    "    out_files={\n",
    "        \"mae\": Path(\"../scores_mahalanobis/scores_mahalanobis_mae.csv\")\n",
    "    },\n",
    "    backbones=[\"mae\"],\n",
    "    k_top_patches=K_TOP_PATCHES,\n",
    "    pca_dim=PCA_DIM,\n",
    "    pca_whitening_power=WHITENING,\n",
    "    shrinkage_alpha=SHRINKAGE,\n",
    "    query_batch_size=BATCH_SIZE,\n",
    "    save_patch_scores=True,\n",
    "    patch_out_root=PATCH_OUT_ROOT,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "842372db-ab22-4edd-b0e7-ae21418be0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DINO :: Mahalanobis scoring ===\n",
      "idx             object\n",
      "category        object\n",
      "path            object\n",
      "raw_label       object\n",
      "label           object\n",
      "image_score    float64\n",
      "dtype: object\n",
      "0    320.992706\n",
      "1    297.331390\n",
      "2    308.148987\n",
      "3    315.113953\n",
      "4    330.248535\n",
      "Name: image_score, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6921.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>418.711800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>214.234865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>157.835602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.797150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>338.385559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>508.260559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1473.911621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_score\n",
       "count  6921.000000\n",
       "mean    418.711800\n",
       "std     214.234865\n",
       "min     157.835602\n",
       "25%     260.797150\n",
       "50%     338.385559\n",
       "75%     508.260559\n",
       "max    1473.911621"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Mean anomaly score per label (cleaned):\n",
      "label\n",
      "defect    482.330091\n",
      "good      245.735683\n",
      "Name: image_score, dtype: float64\n",
      "\n",
      "üìà Difference (defect - good): 236.5944\n",
      "\n",
      "=== MAE :: Mahalanobis scoring ===\n",
      "idx             object\n",
      "category        object\n",
      "path            object\n",
      "raw_label       object\n",
      "label           object\n",
      "image_score    float64\n",
      "dtype: object\n",
      "0    212.623291\n",
      "1    195.192902\n",
      "2    240.066895\n",
      "3    199.570724\n",
      "4    217.294144\n",
      "Name: image_score, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5175.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>310.277570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>294.352502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>67.903641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>169.548019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>231.408295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>353.813065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3728.071045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_score\n",
       "count  5175.000000\n",
       "mean    310.277570\n",
       "std     294.352502\n",
       "min      67.903641\n",
       "25%     169.548019\n",
       "50%     231.408295\n",
       "75%     353.813065\n",
       "max    3728.071045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Mean anomaly score per label (cleaned):\n",
      "label\n",
      "defect    338.13229\n",
      "good      235.24280\n",
      "Name: image_score, dtype: float64\n",
      "\n",
      "üìà Difference (defect - good): 102.8895\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths ---\n",
    "p_dino = Path(\"../scores_mahalanobis/scores_mahalanobis_dino.csv\")\n",
    "p_mae  = Path(\"../scores_mahalanobis/scores_mahalanobis_mae.csv\")\n",
    "\n",
    "def analyze_scores(path: Path, model_name: str):\n",
    "    print(f\"\\n=== {model_name} :: Mahalanobis scoring ===\")\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(path, sep=\",\")\n",
    "    \n",
    "    # Ensure numeric\n",
    "    df[\"image_score\"] = pd.to_numeric(df[\"image_score\"], errors=\"coerce\")\n",
    "    \n",
    "    # Quick type / head check\n",
    "    print(df.dtypes)\n",
    "    print(df[\"image_score\"].head())\n",
    "    \n",
    "    # Statistical summary\n",
    "    display(df.describe()[[\"image_score\"]])\n",
    "    \n",
    "    # Keep only good/defect\n",
    "    mean_scores = (\n",
    "        df[df[\"label\"].isin([\"good\", \"defect\"])]\n",
    "        .groupby(\"label\")[\"image_score\"]\n",
    "        .mean()\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüîç Mean anomaly score per label (cleaned):\")\n",
    "    print(mean_scores)\n",
    "    \n",
    "    # Difference defect - good\n",
    "    if {\"good\", \"defect\"} <= set(mean_scores.index):\n",
    "        delta = mean_scores[\"defect\"] - mean_scores[\"good\"]\n",
    "        print(f\"\\nüìà Difference (defect - good): {delta:.4f}\")\n",
    "\n",
    "# --- Run for both backbones ---\n",
    "analyze_scores(p_dino, \"DINO\")\n",
    "analyze_scores(p_mae,  \"MAE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2afe4b81-60c9-4dfd-95d0-23fe93ed251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Mean anomaly score per label (Mahalanobis):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DINO (Mahalanobis)</th>\n",
       "      <th>MAE (Mahalanobis)</th>\n",
       "      <th>Œî (defect - good) DINO</th>\n",
       "      <th>Œî (defect - good) MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>246.607456</td>\n",
       "      <td>235.242988</td>\n",
       "      <td>233.319498</td>\n",
       "      <td>102.889107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>defect</th>\n",
       "      <td>479.926954</td>\n",
       "      <td>338.132095</td>\n",
       "      <td>233.319498</td>\n",
       "      <td>102.889107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DINO (Mahalanobis)  MAE (Mahalanobis)  Œî (defect - good) DINO  \\\n",
       "label                                                                   \n",
       "good            246.607456         235.242988              233.319498   \n",
       "defect          479.926954         338.132095              233.319498   \n",
       "\n",
       "        Œî (defect - good) MAE  \n",
       "label                          \n",
       "good               102.889107  \n",
       "defect             102.889107  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Score difference (defect - good):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DINO (Mahalanobis)    233.319498\n",
       "MAE (Mahalanobis)     102.889107\n",
       "Name: Œî (defect - good), dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths ---\n",
    "p_dino = Path(\"../scores_mahalanobis/scores_mahalanobis_dino.csv\")\n",
    "p_mae  = Path(\"../scores_mahalanobis/scores_mahalanobis_mae.csv\")\n",
    "\n",
    "# --- Load + clean helper ---\n",
    "def load_clean_scores(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    # Ensure numeric (in case it's stored as string)\n",
    "    if df[\"image_score\"].dtype != \"float\":\n",
    "        df[\"image_score\"] = pd.to_numeric(df[\"image_score\"], errors=\"coerce\")\n",
    "    # Keep only valid labels\n",
    "    df = df[df[\"label\"].isin([\"good\", \"defect\"])].copy()\n",
    "    # Drop corrupted rows if any\n",
    "    df = df.dropna(subset=[\"image_score\", \"label\"])\n",
    "    return df\n",
    "\n",
    "df_dino = load_clean_scores(p_dino)\n",
    "df_mae  = load_clean_scores(p_mae)\n",
    "\n",
    "# --- Mean per label ---\n",
    "mean_dino = df_dino.groupby(\"label\")[\"image_score\"].mean()\n",
    "mean_mae  = df_mae.groupby(\"label\")[\"image_score\"].mean()\n",
    "\n",
    "# Align index to ensure consistent order\n",
    "label_order = [\"good\", \"defect\"]\n",
    "mean_dino = mean_dino.reindex(label_order)\n",
    "mean_mae  = mean_mae.reindex(label_order)\n",
    "\n",
    "# --- Merge + deltas ---\n",
    "mean_df = pd.DataFrame({\n",
    "    \"DINO (Mahalanobis)\": mean_dino,\n",
    "    \"MAE (Mahalanobis)\":  mean_mae\n",
    "})\n",
    "mean_df[\"Œî (defect - good) DINO\"] = mean_dino.loc[\"defect\"] - mean_dino.loc[\"good\"]\n",
    "mean_df[\"Œî (defect - good) MAE\"]  = mean_mae.loc[\"defect\"]  - mean_mae.loc[\"good\"]\n",
    "\n",
    "print(\"üîç Mean anomaly score per label (Mahalanobis):\")\n",
    "display(mean_df)\n",
    "\n",
    "# Compact delta-only output\n",
    "delta = pd.Series({\n",
    "    \"DINO (Mahalanobis)\": mean_dino.loc[\"defect\"] - mean_dino.loc[\"good\"],\n",
    "    \"MAE (Mahalanobis)\":  mean_mae.loc[\"defect\"]  - mean_mae.loc[\"good\"],\n",
    "}, name=\"Œî (defect - good)\")\n",
    "print(\"\\nüìà Score difference (defect - good):\")\n",
    "display(delta)\n",
    "\n",
    "# --- Optional: per-category check (uncomment display to view) ---\n",
    "per_cat = (\n",
    "    pd.concat(\n",
    "        [df_dino.assign(model=\"DINO\"), df_mae.assign(model=\"MAE\")],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    .groupby([\"model\", \"category\", \"label\"])[\"image_score\"].mean()\n",
    "    .unstack(\"label\")\n",
    "    .assign(delta=lambda x: x[\"defect\"] - x[\"good\"])\n",
    "    .sort_values([\"model\", \"delta\"], ascending=[True, False])\n",
    ")\n",
    "# display(per_cat)  # Uncomment if you want to inspect per-category deltas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8d122-f46d-4b35-b75c-fd8077252ec7",
   "metadata": {},
   "source": [
    "### üîé Summary of Results (KNN vs. Mahalanobis)\n",
    "\n",
    "We evaluated anomaly scores using two backbones (**DINO** and **MAE**) with two distance metrics (**KNN** and **Mahalanobis**).  \n",
    "The table below shows the mean anomaly score for *good* and *defect* samples, as well as the difference between them (defect ‚Äì good).  \n",
    "A larger difference indicates a clearer separation between normal and anomalous samples.\n",
    "\n",
    "| Metric         | Model | Good (‚Üì better) | Defect (‚Üë better) | Œî (defect ‚Äì good) |\n",
    "|----------------|-------|-----------------|-------------------|-------------------|\n",
    "| **KNN**        | DINO  | 0.097           | 0.207             | **0.110** |\n",
    "|                | MAE   | 0.149           | 0.183             | 0.035 |\n",
    "| **Mahalanobis**| DINO  | 246.6           | 479.9             | **233.3** |\n",
    "|                | MAE   | 235.2           | 338.1             | 102.9 |\n",
    "\n",
    "#### Key observations\n",
    "- **DINO consistently outperforms MAE** in both metrics, achieving a larger separation (Œî) between good and defect.\n",
    "- **Mahalanobis yields higher absolute score ranges** than KNN, but the relative separation is what matters ‚Äî DINO again provides the clearest gap.\n",
    "- **MAE shows weaker separation**, especially with KNN, which may limit its discriminative power for anomaly detection.\n",
    "\n",
    "#### Next step: Heatmaps\n",
    "While global scores provide evidence that DINO (especially with Mahalanobis) is more discriminative, they do not reveal *where* anomalies occur.  \n",
    "The next step is **pixel-level heatmap generation**:\n",
    "- Map patch-level distances back to the image grid (e.g., 16√ó16 feature map).\n",
    "- Upsample to the original resolution.\n",
    "- Overlay as heatmaps to visualize **localized defects**.\n",
    "\n",
    "This will allow a **qualitative evaluation** and demonstrate whether the quantitative advantage of DINO also translates into clearer anomaly localization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47942ab4-2e2f-49f4-94db-d00cc4b7b5d6",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Heatmap Generation (Pixel-Level)\n",
    "\n",
    "While image-level anomaly scores provide a single value per image (indicating whether it is normal or defective), they do not reveal **where** the anomaly occurs.  \n",
    "To gain deeper insights, we generate **pixel-level heatmaps** that localize potential defects within the image.\n",
    "\n",
    "The core idea is straightforward:\n",
    "\n",
    "1. **Patch-level distances**: Each test image is split into patches (e.g., 16√ó16 grid for a 256√ó256 image). Distances between these patches and the normal feature distribution serve as local anomaly scores.\n",
    "2. **Interpolation**: The patch-level grid is upsampled (e.g., bilinear interpolation) to match the original image resolution.\n",
    "3. **Visualization**: The resulting map can be rendered as a heatmap or overlayed on the original image, clearly highlighting defective regions.\n",
    "\n",
    "This step transforms anomaly detection from a **binary decision** into an interpretable, **explainable localization task** ‚Äî essential for real-world quality inspection scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d6288e6-3b70-4836-b629-3a0991e76796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category     | CSV rows | NPZ files | Match?\n",
      "---------------------------------------------\n",
      "bottle       |       83 |        22 | False\n",
      "cable        |      150 |        58 | False\n",
      "capsule      |      132 |        23 | False\n",
      "carpet       |      117 |        28 | False\n",
      "grid         |       78 |        21 | False\n",
      "hazelnut     |      110 |        40 | False\n",
      "leather      |      124 |        32 | False\n",
      "metal_nut    |      115 |        25 | False\n",
      "pill         |      167 |        26 | False\n",
      "screw        |      160 |        41 | False\n",
      "tile         |      117 |        33 | False\n",
      "toothbrush   |       42 |        30 | False\n",
      "transistor   |      100 |        60 | False\n",
      "wood         |       79 |        21 | False\n",
      "zipper       |      151 |        32 | False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_FILE = Path(\"../scores_mahalanobis/scores_mahalanobis_dino.csv\")\n",
    "PATCH_DIR = Path(\"../scores_mahalanobis/patch_scores/dino\")\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "summary = []\n",
    "for cat, group in df.groupby(\"category\"):\n",
    "    csv_count = len(group)\n",
    "    npz_count = len(list((PATCH_DIR / cat).glob(\"*.npz\")))\n",
    "    summary.append((cat, csv_count, npz_count, csv_count == npz_count))\n",
    "\n",
    "# Print results\n",
    "print(f\"{'Category':12s} | {'CSV rows':8s} | {'NPZ files':9s} | Match?\")\n",
    "print(\"-\"*45)\n",
    "for cat, csv_c, npz_c, ok in summary:\n",
    "    print(f\"{cat:12s} | {csv_c:8d} | {npz_c:9d} | {ok}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba64aee-0afe-4f9b-9722-38e71a364302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mvtec)",
   "language": "python",
   "name": "mvtec-vit-anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
